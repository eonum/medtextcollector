{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Medtext Pipeline\n",
    "\n",
    "Assembling, parsing, crawling of all components to build the medtext databases. Further information can be found in the wiki: http://wiki.eonum.ch/doku.php?id=data:medicaltext\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import load_config\n",
    "import json\n",
    "\n",
    "### define a new configuration\n",
    "config_dict = {\n",
    "    # databases\n",
    "    \"medical_books_pdf_folder\": \"/media/data/medical_books/medical_books_pdf\",\n",
    "    \"medical_books_txt_folder\": \"/media/data/medical_books/medical_books_plaintxt/\",\n",
    "    \"wiki_dump_url\": \"https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2\",\n",
    "    \"wiki_dump\": \"/media/data/wiki_dumps/dewiki-latest-pages-articles.xml.bz2\",\n",
    "    \"wiki_dump_extracted\": \"/media/data/wiki_dumps/dewiki-latest-pages-articles\",\n",
    "    \"wiki_medical_txt\": \"/media/data/wiki_dumps/wiki_med_txts\",\n",
    "    \"wiki_non_medical_txt\": \"/media/data/wiki_dumps/wiki_non_med_txts\",\n",
    "    \"crawler_output_html\": \"/media/data/medtextcollector/data/output/crawler/raw\",\n",
    "    \"crawler_output_txt\": \"/media/data/medtextcollector/data/output/crawler/pages\",\n",
    "    \n",
    "    # models\n",
    "    \"medtext_classifier_config\": \"\",\n",
    "    \"embedding_model\": \"\",\n",
    "    \n",
    "    # chose tokenizer. possible values: \"nst\" and \"sgt\"\n",
    "    ## NonStemmingTokenizer: 'nst'\n",
    "    # - no stemming, only remove punctuation marks\n",
    "    # - lowercase letters\n",
    "\n",
    "    ## SimpleGermanTokenizer: 'sgt'\n",
    "    # - remove punctuation marks\n",
    "    # - stemming\n",
    "    # - lowercase letters\n",
    "    \"tokenizer\": \"nst\",\n",
    "    \n",
    "    # where to store the configuration file\n",
    "    \"config_path\": \"/media/data/configuration-medtext-notebook-pipeline.json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### save config file (don't change)\n",
    "config_src = config_dict[\"config_path\"]\n",
    "\n",
    "with open(config_src, 'w+') as f:\n",
    "    json.dump(config_dict, f, indent=4)\n",
    "    \n",
    "# load config object based on config file (don't change)\n",
    "config = load_config.Configuration(config_src, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dies', 'ist', 'ein', 'beispielastz']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load sentence detector and tokenizer\n",
    "\n",
    "import nltk\n",
    "from notebooks.tokenizer import get_tokenizer\n",
    "\n",
    "sentence_detector = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "tokenizer = get_tokenizer(config['tokenizer'])\n",
    "\n",
    "sentence_detector.tokenize(\"Dies ist ein Beispielsatz! Und hier noch einmal.\\nNeuer Absatz: Aber hallo.. Du\")\n",
    "\n",
    "tokenizer.tokenize(\"Dies ist ein Beispielastz!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data bases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>database                                            </td><td>files</td></tr>\n",
       "<tr><td>/media/data/medical_books/medical_books_pdf         </td><td>53869</td></tr>\n",
       "<tr><td>/media/data/wiki_dumps/dewiki-latest-pages-articles </td><td>0    </td></tr>\n",
       "<tr><td>/media/data/medtextcollector/data/output/crawler/raw</td><td>8009 </td></tr>\n",
       "<tr><td>/media/data/wiki_dumps/wiki_non_med_txts            </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data bases\n",
      "['/media/data/medical_books/medical_books_plaintxt/', 22894, 22881, 8301228, 104682611, 2101323]\n",
      "['/media/data/wiki_dumps/wiki_med_txts', 10485, 10485, 249609, 5194030, 395226]\n",
      "['/media/data/medtextcollector/data/output/crawler/pages', 29666, 29666, 3805554, 65325417, 481399]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>database                                              </td><td>files </td><td>documents</td><td>sentences</td><td>tokens     </td><td>vocabulary</td></tr>\n",
       "<tr><td>/media/data/medical_books/medical_books_plaintxt/     </td><td>22,894</td><td>22,881   </td><td>8,301,228</td><td>104,682,611</td><td>2,101,323 </td></tr>\n",
       "<tr><td>/media/data/wiki_dumps/wiki_med_txts                  </td><td>10,485</td><td>10,485   </td><td>249,609  </td><td>5,194,030  </td><td>395,226   </td></tr>\n",
       "<tr><td>/media/data/medtextcollector/data/output/crawler/pages</td><td>29,666</td><td>29,666   </td><td>3,805,554</td><td>65,325,417 </td><td>481,399   </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union vocabulary2495233\n",
      "Intersection vocabulary125415\n"
     ]
    }
   ],
   "source": [
    "# get number of files, sentences, tokens, vocabulary and cahracter histograms in each database. \n",
    "# This cell takes quite a while to execute!\n",
    "from data_analysis_toolkit import get_files_from_folder, load_documents , extract_sentences\n",
    "\n",
    "databases_raw = [config[\"medical_books_pdf_folder\"], config[\"wiki_dump_extracted\"], config[\"crawler_output_html\"],\n",
    "                config[\"wiki_non_medical_txt\"], ]\n",
    "databases_cleaned = [config[\"medical_books_txt_folder\"], config[\"wiki_medical_txt\"], \n",
    "                     config[\"crawler_output_txt\"]]\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "table = [[\"database\", \"files\"]]\n",
    "\n",
    "print(\"Raw data bases\")\n",
    "for db in  databases_raw:\n",
    "    table.append([db, len(get_files_from_folder(db))])\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "print(\"Cleaned data bases\")\n",
    "table = [[\"database\", \"files\", \"documents\", \"sentences\", \"tokens\", \"vocabulary\"]]\n",
    "vocs = []\n",
    "for db in  databases_cleaned:\n",
    "    files = get_files_from_folder(db)\n",
    "    documents, load_errors = load_documents(files)\n",
    "    sentences = []\n",
    "    tokens = []\n",
    "    for document in documents:\n",
    "        sentences.extend(extract_sentences(document, sentence_detector))\n",
    "    for sentence in sentences:\n",
    "        tokens.extend(tokenizer.tokenize(sentence))\n",
    "    voc = set(tokens)\n",
    "    vocs.append(voc)\n",
    "    print([db, len(files), len(documents), len(sentences), len(tokens), len(voc)])\n",
    "    table.append([db, \"{:,}\".format(len(files)), \"{:,}\".format(len(documents)), \n",
    "                  \"{:,}\".format(len(sentences)), \"{:,}\".format(len(tokens)), \n",
    "                  \"{:,}\".format(len(voc))])\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "voc_union = vocs[0]\n",
    "voc_intersection = vocs[0]\n",
    "for voc in vocs:\n",
    "    voc_union = voc_union.union(voc)\n",
    "    voc_intersection = voc_intersection.intersection(voc)\n",
    "\n",
    "print(\"Union vocabulary: \" + \"{:,}\".format(len(voc_union)))\n",
    "print(\"Intersection vocabulary: \" + \"{:,}\".format(len(voc_intersection)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module.__init__() takes at most 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-d74ec9b818ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from tqdm import tnrange, tqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTqdmUpTo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"Provides `update_to(n)` which uses `tqdm.update(delta_n)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: module.__init__() takes at most 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "# download and extract wiki dump\n",
    "import urllib.request\n",
    "\n",
    "import tqdm\n",
    "#from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "class TqdmUpTo(tqdm):\n",
    "    \"\"\"Provides `update_to(n)` which uses `tqdm.update(delta_n)`.\"\"\"\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        \"\"\"\n",
    "        b  : int, optional\n",
    "            Number of blocks transferred so far [default: 1].\n",
    "        bsize  : int, optional\n",
    "            Size of each block (in tqdm units) [default: 1].\n",
    "        tsize  : int, optional\n",
    "            Total size (in tqdm units). If [default: None] remains unchanged.\n",
    "        \"\"\"\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)  # will also set self.n = b * bsize\n",
    "\n",
    "with TqdmUpTo() as t:  # all optional kwargs\n",
    "    urllib.request.urlretrieve(config[\"wiki_dump_url\"], config[\"wiki_dump\"], reporthook=t.update_to, data=None)\n",
    "                                  \n",
    "\n",
    "                         \n",
    "                         \n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract medical and non medical articles from wiki dump and convert XML to TXT\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train medtext classifier\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start crawler\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract TXT from PDFs\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train embedding / start medword\n",
    "\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
