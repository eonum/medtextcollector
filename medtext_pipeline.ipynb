{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Medtext Pipeline\n",
    "\n",
    "Assembling, parsing, crawling of all components to build the medtext databases. Further information can be found in the wiki: http://wiki.eonum.ch/doku.php?id=data:medicaltext\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import load_config\n",
    "import json\n",
    "\n",
    "### define a new configuration\n",
    "config_dict = {\n",
    "    # databases\n",
    "    \"medical_books_pdf_folder\": \"/media/data/medical_books/medical_books_pdf\",\n",
    "    \"medical_books_txt_folder\": \"/media/data/medical_books/medical_books_plaintxt/\",\n",
    "    \"wiki_dump_url\": \"https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2\",\n",
    "    \"wiki_dump\": \"/media/data/wiki_dumps/dewiki-latest-pages-articles.xml.bz2\",\n",
    "    \"wiki_dump_extracted\": \"/media/data/wiki_dumps/dewiki-latest-pages-articles\",\n",
    "    \"wiki_medical_txt\": \"/media/data/wiki_dumps/wiki_med_txts\",\n",
    "    \"wiki_non_medical_txt\": \"/media/data/wiki_dumps/wiki_non_med_txts\",\n",
    "    \"crawler_output_html\": \"/media/data/medtextcollector/data/output/crawler/raw\",\n",
    "    \"crawler_output_txt\": \"/media/data/medtextcollector/data/output/crawler/pages\",\n",
    "    \n",
    "    # models\n",
    "    \"medtext_classifier_config\": \"\",\n",
    "    \"embedding_model\": \"\",\n",
    "    \n",
    "    # chose tokenizer. possible values: \"nst\" and \"sgt\"\n",
    "    ## NonStemmingTokenizer: 'nst'\n",
    "    # - no stemming, only remove punctuation marks\n",
    "    # - lowercase letters\n",
    "\n",
    "    ## SimpleGermanTokenizer: 'sgt'\n",
    "    # - remove punctuation marks\n",
    "    # - stemming\n",
    "    # - lowercase letters\n",
    "    \"tokenizer\": \"nst\",\n",
    "    \n",
    "    # where to store the configuration file\n",
    "    \"config_path\": \"/media/data/configuration-medtext-notebook-pipeline.json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### save config file (don't change)\n",
    "config_src = config_dict[\"config_path\"]\n",
    "\n",
    "with open(config_src, 'w+') as f:\n",
    "    json.dump(config_dict, f, indent=4)\n",
    "    \n",
    "# load config object based on config file (don't change)\n",
    "config = load_config.Configuration(config_src, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dies', 'ist', 'ein', 'Beispielsatz!']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence detection\n",
    "\n",
    "import nltk\n",
    "sentence_detector = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "\n",
    "sentence_detector.tokenize(\"Dies ist ein Beispielsatz! Und hier noch einmal.\\nNeuer Absatz: Aber hallo.. Du\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data bases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>database                                            </td><td>files</td></tr>\n",
       "<tr><td>/media/data/medical_books/medical_books_pdf         </td><td>53869</td></tr>\n",
       "<tr><td>/media/data/wiki_dumps/dewiki-latest-pages-articles </td><td>0    </td></tr>\n",
       "<tr><td>/media/data/medtextcollector/data/output/crawler/raw</td><td>8009 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data bases\n"
     ]
    }
   ],
   "source": [
    "# get number of files, sentences, tokens, distinct tokens in each database\n",
    "from data_analysis_toolkit import get_files_from_folder, load_documents , extract_sentences\n",
    "\n",
    "databases_raw = [config[\"medical_books_pdf_folder\"], config[\"wiki_dump_extracted\"], config[\"crawler_output_html\"]]\n",
    "databases_cleaned = [config[\"medical_books_txt_folder\"], config[\"wiki_medical_txt\"], \n",
    "                     config[\"wiki_non_medical_txt\"], config[\"crawler_output_txt\"]]\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "table = [[\"database\", \"files\"]]\n",
    "\n",
    "print(\"Raw data bases\")\n",
    "for db in  databases_raw:\n",
    "    table.append([db, len(get_files_from_folder(db))])\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "print(\"Cleaned data bases\")\n",
    "table = [[\"database\", \"files\", \"sentences\", \"tokens\", \"vocabulary\"]]\n",
    "for db in  databases_cleaned:\n",
    "    files = get_files_from_folder(db)\n",
    "    documents, load_errors = load_documents(files)\n",
    "    sentences = []\n",
    "    for document in documents:\n",
    "        sentences.extend(extract_sentences(document, sentence_detector))\n",
    "    table.append([db, len(files), len(sentences)])\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download and extract wiki dump\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract medical and non medical articles from wiki dump and convert XML to TXT\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train medtext classifier\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start crawler\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract TXT from PDFs\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train embedding / start medword\n",
    "\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
