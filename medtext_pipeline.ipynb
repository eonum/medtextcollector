{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Medtext Pipeline\n",
    "\n",
    "Assembling, parsing, crawling of all components to build the medtext databases. Further information can be found in the wiki: http://wiki.eonum.ch/doku.php?id=data:medicaltext\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import load_config\n",
    "import json\n",
    "\n",
    "### define a new configuration\n",
    "config_dict = {\n",
    "    # databases\n",
    "    \"medical_books_pdf_folder\": \"/media/data/medical_books/medical_books_pdf\",\n",
    "    \"medical_books_txt_folder\": \"/media/data/medical_books/medical_books_plaintxt/\",\n",
    "    \"wiki_dump_url\": \"https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2\",\n",
    "    \"wiki_dump\": \"/media/data/wiki_dumps/dewiki-latest-pages-articles.xml.bz2\",\n",
    "    \"wiki_dump_extracted\": \"/media/data/wiki_dumps/dewiki-latest-pages-articles\",\n",
    "    \"wiki_medical_txt\": \"/media/data/wiki_dumps/wiki_med_txts\",\n",
    "    \"wiki_non_medical_txt\": \"\",\n",
    "    \"crawler_output_html\": \"/media/data/medtextcollector/data/output/crawler/raw\",\n",
    "    \"crawler_output_txt\": \"/media/data/medtextcollector/data/output/crawler/pages\",\n",
    "    \n",
    "    # models\n",
    "    \"medtext_classifier_config\": \"\",\n",
    "    \"embedding_model\": \"\",\n",
    "    \n",
    "    # chose tokenizer. possible values: \"nst\" and \"sgt\"\n",
    "    ## NonStemmingTokenizer: 'nst'\n",
    "    # - no stemming, only remove punctuation marks\n",
    "    # - lowercase letters\n",
    "\n",
    "    ## SimpleGermanTokenizer: 'sgt'\n",
    "    # - remove punctuation marks\n",
    "    # - stemming\n",
    "    # - lowercase letters\n",
    "    \"tokenizer\": \"nst\",\n",
    "    \n",
    "    # where to store the configuration file\n",
    "    \"config_path\": \"/media/data/configuration-medtext-notebook-pipeline.json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### save config file (don't change)\n",
    "config_src = config_dict[\"config_path\"]\n",
    "\n",
    "with open(config_src, 'w+') as f:\n",
    "    json.dump(config_dict, f, indent=4)\n",
    "    \n",
    "# load config object based on config file (don't change)\n",
    "config = load_config.Configuration(config_src, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get number of files, sentences and tokens in each database\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download and extract wiki dump\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract medical and non medical articles from wiki dump and convert XML to TXT\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train medtext classifier\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start crawler\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract TXT from PDFs\n",
    "\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
