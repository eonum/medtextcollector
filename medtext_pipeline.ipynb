{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Medtext Pipeline\n",
    "\n",
    "Assembling, parsing, crawling of all components to build the medtext databases. Further information can be found in the wiki: http://wiki.eonum.ch/doku.php?id=data:medicaltext\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import load_config\n",
    "import json\n",
    "\n",
    "### define a new configuration\n",
    "config_dict = {\n",
    "    # databases\n",
    "    \"medical_books_pdf_folder\": \"/media/data/medical_books/medical_books_pdf\",\n",
    "    \"medical_books_txt_folder\": \"/media/data/medical_books/medical_books_plaintxt/\",\n",
    "    \"wiki_dump_url\": \"https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xml.bz2\",\n",
    "    \"wiki_dump\": \"/media/data/wiki_dumps/dewiki-latest-pages-articles.xml.bz2\",\n",
    "    \"categories_file\": \"/media/data/wiki_dumps/categories\",\n",
    "    \"categories_dump_file\": \"../wiki_dumps/dewiki-latest-category.sql\",\n",
    "    \"wiki_dump_extracted\": \"/media/data/wiki_dumps/dewiki-latest-pages-articles\",\n",
    "    \"wiki_medical_txt\": \"/media/data/wiki_dumps/wiki_med_txts\",\n",
    "    \"wiki_non_medical_txt\": \"/media/data/wiki_dumps/wiki_non_med_txts\",\n",
    "    \"crawler_output_html\": \"/media/data/medtextcollector/data/output/crawler/raw\",\n",
    "    \"crawler_output_txt\": \"/media/data/medtextcollector/data/output/crawler/pages\",\n",
    "    \n",
    "    # models\n",
    "    \"medtext_classifier_config\": \"\",\n",
    "    \"embedding_model\": \"\",\n",
    "    \n",
    "    # chose tokenizer. possible values: \"nst\" and \"sgt\"\n",
    "    ## NonStemmingTokenizer: 'nst'\n",
    "    # - no stemming, only remove punctuation marks\n",
    "    # - lowercase letters\n",
    "\n",
    "    ## SimpleGermanTokenizer: 'sgt'\n",
    "    # - remove punctuation marks\n",
    "    # - stemming\n",
    "    # - lowercase letters\n",
    "    \"tokenizer\": \"nst\",\n",
    "    \n",
    "    # where to store the configuration file\n",
    "    \"config_path\": \"/media/data/configuration-medtext-notebook-pipeline.json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### save config file (don't change)\n",
    "config_src = config_dict[\"config_path\"]\n",
    "\n",
    "with open(config_src, 'w+') as f:\n",
    "    json.dump(config_dict, f, indent=4)\n",
    "    \n",
    "# load config object based on config file (don't change)\n",
    "config = load_config.Configuration(config_src, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dies ist ein Beispielsatz!', 'Und hier noch einmal.', 'Neuer Absatz: Aber hallo.. Du Und ein Beispiel z.B. oder z.Bsp: Ein Beispiel.', 'J. P.', 'Klein (1), S. Moritz (2), T. Berger (3)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dies',\n",
       " 'ist',\n",
       " 'ein',\n",
       " 'beispielastz',\n",
       " 'komm',\n",
       " 'wir',\n",
       " 'machen',\n",
       " 'eine',\n",
       " 'überraschung',\n",
       " 'und',\n",
       " 'stemmen',\n",
       " 'diesen',\n",
       " 'satz',\n",
       " 'p',\n",
       " '987',\n",
       " 'à',\n",
       " 'è']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load sentence detector and tokenizer\n",
    "\n",
    "import nltk\n",
    "from notebooks.tokenizer import get_tokenizer\n",
    "from data_analysis_toolkit import extract_sentences\n",
    "\n",
    "sentence_detector = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "tokenizer = get_tokenizer(config_dict['tokenizer'])\n",
    "\n",
    "print(extract_sentences(\"Dies ist ein Beispielsatz! Und hier noch einmal.\\nNeuer Absatz: Aber hallo.. Du\"\n",
    "                                 + \"\\nUnd ein Beispiel z.B. oder z.Bsp: Ein Beispiel. \" +\n",
    "                        \"J. P. Klein (1), S. Moritz (2), T. Berger (3)\", sentence_detector))\n",
    "\n",
    "tokenizer.tokenize(\"Dies ist ein Beispielastz! Komm wir machen eine Überraschung und stemmen diesen Satz.\\n p 987*/-_à!è\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data bases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>database                                            </td><td>files</td></tr>\n",
       "<tr><td>/media/data/medical_books/medical_books_pdf         </td><td>53869</td></tr>\n",
       "<tr><td>/media/data/wiki_dumps/dewiki-latest-pages-articles </td><td>0    </td></tr>\n",
       "<tr><td>/media/data/medtextcollector/data/output/crawler/raw</td><td>8009 </td></tr>\n",
       "<tr><td>/media/data/wiki_dumps/wiki_non_med_txts            </td><td>0    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data bases\n",
      "['/media/data/medical_books/medical_books_plaintxt/', 22894, 22881, 8301228, 104682611, 2101323]\n",
      "['/media/data/wiki_dumps/wiki_med_txts', 10485, 10485, 249609, 5194030, 395226]\n",
      "['/media/data/medtextcollector/data/output/crawler/pages', 29666, 29666, 3805554, 65325417, 481399]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>database                                              </td><td>files </td><td>documents</td><td>sentences</td><td>tokens     </td><td>vocabulary</td></tr>\n",
       "<tr><td>/media/data/medical_books/medical_books_plaintxt/     </td><td>22,894</td><td>22,881   </td><td>8,301,228</td><td>104,682,611</td><td>2,101,323 </td></tr>\n",
       "<tr><td>/media/data/wiki_dumps/wiki_med_txts                  </td><td>10,485</td><td>10,485   </td><td>249,609  </td><td>5,194,030  </td><td>395,226   </td></tr>\n",
       "<tr><td>/media/data/medtextcollector/data/output/crawler/pages</td><td>29,666</td><td>29,666   </td><td>3,805,554</td><td>65,325,417 </td><td>481,399   </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union vocabulary: 2,495,233\n",
      "Intersection vocabulary: 125,415\n"
     ]
    }
   ],
   "source": [
    "# get number of files, sentences, tokens, vocabulary and character histograms in each database. \n",
    "# This cell takes quite a while to execute!\n",
    "from data_analysis_toolkit import get_files_from_folder, load_documents\n",
    "\n",
    "databases_raw = [config[\"medical_books_pdf_folder\"], config[\"wiki_dump_extracted\"], config[\"crawler_output_html\"],\n",
    "                config[\"wiki_non_medical_txt\"], ]\n",
    "databases_cleaned = [config[\"medical_books_txt_folder\"], config[\"wiki_medical_txt\"], \n",
    "                     config[\"crawler_output_txt\"]]\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "table = [[\"database\", \"files\"]]\n",
    "\n",
    "print(\"Raw data bases\")\n",
    "for db in  databases_raw:\n",
    "    table.append([db, len(get_files_from_folder(db))])\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "print(\"Cleaned data bases\")\n",
    "table = [[\"database\", \"files\", \"documents\", \"sentences\", \"tokens\", \"vocabulary\"]]\n",
    "vocs = []\n",
    "docs = []\n",
    "for db in  databases_cleaned:\n",
    "    files = get_files_from_folder(db)\n",
    "    documents, load_errors = load_documents(files)\n",
    "    sentences = []\n",
    "    tokens = []\n",
    "    docs.append(documents)\n",
    "    for document in documents:\n",
    "        sentences.extend(extract_sentences(document, sentence_detector))\n",
    "    for sentence in sentences:\n",
    "        tokens.extend(tokenizer.tokenize(sentence))\n",
    "    voc = set(tokens)\n",
    "    vocs.append(voc)\n",
    "    print([db, len(files), len(documents), len(sentences), len(tokens), len(voc)])\n",
    "    table.append([db, \"{:,}\".format(len(files)), \"{:,}\".format(len(documents)), \n",
    "                  \"{:,}\".format(len(sentences)), \"{:,}\".format(len(tokens)), \n",
    "                  \"{:,}\".format(len(voc))])\n",
    "    \n",
    "    # TODO print character histograms\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "voc_union = vocs[0]\n",
    "voc_intersection = vocs[0]\n",
    "for voc in vocs:\n",
    "    voc_union = voc_union.union(voc)\n",
    "    voc_intersection = voc_intersection.intersection(voc)\n",
    "\n",
    "print(\"Union vocabulary: \" + \"{:,}\".format(len(voc_union)))\n",
    "print(\"Intersection vocabulary: \" + \"{:,}\".format(len(voc_intersection)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Document ==\n",
      "Leitthema\n",
      "Diabetologe 2008 · 4:416–424\n",
      "DOI 10.1007/s11428-008-0252-5\n",
      "Online publiziert: 27. Juli 2008\n",
      "© Springer Medizin Verlag 2008\n",
      "\n",
      "P. Rösen\n",
      "Deutsches Diabetesforschungszentrum, Düsseldorf\n",
      "\n",
      "Die Mehrzahl der Patienten mit Diabetes mellitus verstirbt an akuten thrombotischen Komplikationen vorbestehender\n",
      "Gefäßläsionen. Die hämostaseologischen\n",
      "Kontrollmechanismen haben somit eine unmittelbare Bedeutung für die klinische Prognose des Patienten, insbesondere dann, wenn wie im Fall des Diabetes\n",
      "dami\n",
      "== Sentences ==\n",
      "Leitthema Diabetologe 2008 · 4:416–424 DOI 10.1007/s11428-008-0252-5 Online publiziert: 27. Juli 2008 © Springer Medizin Verlag 2008  P. Rösen Deutsches Diabetesforschungszentrum, Düsseldorf  Die Mehrzahl der Patienten mit Diabetes mellitus verstirbt an akuten thrombotischen Komplikationen vorbestehender Gefäßläsionen.\n",
      "Die hämostaseologischen Kontrollmechanismen haben somit eine unmittelbare Bedeutung für die klinische Prognose des Patienten, insbesondere dann, wenn wie im Fall des Diabetes damit zu rechnen ist, dass sich vaskuläre Komplikationen in Form von mikro- oder auch makroangiopathischen Läsionen bereits frühzeitig aufgrund von Stoffwechseldefekten entwickeln.\n",
      "Das Verstehen und die therapeutische Beeinflussung der komplexen hämostaseologischen Mechanismen können somit von grundlegender Bedeutung für das vaskuläre Schicksal des Patienten sein [5].\n",
      "Gerinnung und Fibrinolyse werden durch ein komplexes System von Wechselwirkungen zwischen Faktoren und Zellen im Blutplasma, der Gefäßoberfläche und der subzellulären, subendothelialen Matrix kontrolliert, wobei lokale, örtlich begrenzte Prozesse im Vordergrund stehen [13, 19].\n",
      "== Tokens ==\n",
      "['leitthema', 'diabetologe', '2008', '', '4416424', 'doi', '101007', 's11428-008-0252-5', 'online', 'publiziert', '27', 'juli', '2008', '', 'springer', 'medizin', 'verlag', '2008', 'p', 'rösen', 'deutsches', 'diabetesforschungszentrum', 'düsseldorf', 'die', 'mehrzahl', 'der', 'patienten', 'mit', 'diabetes', 'mellitus', 'verstirbt', 'an', 'akuten', 'thrombotischen', 'komplikationen', 'vorbestehender', 'gefässläsionen']\n",
      "['die', 'hämostaseologischen', 'kontrollmechanismen', 'haben', 'somit', 'eine', 'unmittelbare', 'bedeutung', 'für', 'die', 'klinische', 'prognose', 'des', 'patienten', 'insbesondere', 'dann', 'wenn', 'wie', 'im', 'fall', 'des', 'diabetes', 'damit', 'zu', 'rechnen', 'ist', 'dass', 'sich', 'vaskuläre', 'komplikationen', 'in', 'form', 'von', 'mikro', 'oder', 'auch', 'makroangiopathischen', 'läsionen', 'bereits', 'frühzeitig', 'aufgrund', 'von', 'stoffwechseldefekten', 'entwickeln']\n",
      "['das', 'verstehen', 'und', 'die', 'therapeutische', 'beeinflussung', 'der', 'komplexen', 'hämostaseologischen', 'mechanismen', 'können', 'somit', 'von', 'grundlegender', 'bedeutung', 'für', 'das', 'vaskuläre', 'schicksal', 'des', 'patienten', 'sein', '5']\n",
      "['gerinnung', 'und', 'fibrinolyse', 'werden', 'durch', 'ein', 'komplexes', 'system', 'von', 'wechselwirkungen', 'zwischen', 'faktoren', 'und', 'zellen', 'im', 'blutplasma', 'der', 'gefässoberfläche', 'und', 'der', 'subzellulären', 'subendothelialen', 'matrix', 'kontrolliert', 'wobei', 'lokale', 'örtlich', 'begrenzte', 'prozesse', 'im', 'vordergrund', 'stehen', '13', '19']\n",
      "\n",
      "== Document ==\n",
      "Albert Hoffa Datei:Albert Hoffa.jpg|Albert Hoffa|miniAlbert Hoffa (* 31. März 1859 in Richmond, Nordkap (Provinz); † 31. Dezember 1907 in Köln) war ein deutscher Chirurg und Orthopäde. Leben Hoffas Vater war Moritz Hoffa, der aus Kassel stammende erste deutsche Arzt in Pretoria. Die Mutter war Mathilde Hoffa geb. Lelienfeld.Nach dem Abitur in Kassel studierte Hoffa Medizin an der Philipps-Universität Marburg. 1879 wurde er im Corps Hasso-Nassovia aktiv. Als Inaktiver wechselte er an die Albert-L\n",
      "== Sentences ==\n",
      "Albert Hoffa Datei:Albert Hoffa.jpg|Albert Hoffa|miniAlbert Hoffa (* 31. März 1859 in Richmond, Nordkap (Provinz); † 31. Dezember 1907 in Köln) war ein deutscher Chirurg und Orthopäde.\n",
      "Leben Hoffas Vater war Moritz Hoffa, der aus Kassel stammende erste deutsche Arzt in Pretoria.\n",
      "Die Mutter war Mathilde Hoffa geb.\n",
      "Lelienfeld.Nach dem Abitur in Kassel studierte Hoffa Medizin an der Philipps-Universität Marburg.\n",
      "== Tokens ==\n",
      "['albert', 'hoffa', 'datei', 'albert', 'hoffajpg', 'albert', 'hoffa', 'minialbert', 'hoffa', '31', 'märz', '1859', 'in', 'richmond', 'nordkap', 'provinz', '†', '31', 'dezember', '1907', 'in', 'köln', 'war', 'ein', 'deutscher', 'chirurg', 'und', 'orthopäde']\n",
      "['leben', 'hoffas', 'vater', 'war', 'moritz', 'hoffa', 'der', 'aus', 'kassel', 'stammende', 'erste', 'deutsche', 'arzt', 'in', 'pretoria']\n",
      "['die', 'mutter', 'war', 'mathilde', 'hoffa', 'geb']\n",
      "['lelienfeldnach', 'dem', 'abitur', 'in', 'kassel', 'studierte', 'hoffa', 'medizin', 'an', 'der', 'philipps-universität', 'marburg']\n",
      "\n",
      "== Document ==\n",
      "\"http://www.sprechzimmer.ch/sprechzimmer/Symptome/Begriff.php?Hautausschlag&kwid=3-88&tid=2\";\"10-07-17-23\";\"pipeline-12-05-17-14\";\"0.990341450874\"\n",
      "\n",
      "   \n",
      "    \n",
      "     \n",
      "      \n",
      "\n",
      "\t\n",
      "\t\n",
      "\t\t\t\t\n",
      "\n",
      "\n",
      "\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t\t\n",
      "\n",
      "\t\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hautausschlag, Ausschlag, Exanthem, Fleckiger, Hautrötung, Rote Hautflecken\n",
      "\n",
      "Definition\n",
      "\n",
      "\t\t\n",
      "\t\t\n",
      "\t\tHautausschlag kann auf eine entzündliche, infektiöse oder allergische Rekation sein\n",
      "\t\t\n",
      "\t\tUnter einem Hautausschlag versteht man eine meist fleckige Rötung, häufig kombiniert mit einer lo\n",
      "== Sentences ==\n",
      "\"http://www.sprechzimmer.ch/sprechzimmer/Symptome/Begriff.php?Hautausschlag&kwid=3-88&tid=2\";\"10-07-17-23\";\"pipeline-12-05-17-14\";\"0.990341450874\"                         \t \t \t\t\t\t   \t\t\t\t \t\t\t\t\t \t\t\t\t\t\t  \t\t\t\t\t\t          Hautausschlag, Ausschlag, Exanthem, Fleckiger, Hautrötung, Rote Hautflecken  Definition  \t\t \t\t \t\tHautausschlag kann auf eine entzündliche, infektiöse oder allergische Rekation sein \t\t \t\tUnter einem Hautausschlag versteht man eine meist fleckige Rötung, häufig kombiniert mit einer lokalen Verdickung der Haut.\n",
      "Rötung und Verdickung sind Zeichen einer Entzündungsreaktion, entweder durch eine Infektion mit einem Mikroorganismus oder durch eine allergische, pseudoallergische oder toxische Reaktion - mit oder ohne Juckreiz.\n",
      "In der Fachsprache spricht man von einem Exanthem, wenn die Haut und von einem Enanthem, wenn die Schleimhaut betroffen ist.\n",
      "Wie äussert sich das Symptom?\n",
      "== Tokens ==\n",
      "['http', '', '', 'wwwsprechzimmerch', 'sprechzimmer', 'symptome', 'begriffphp', 'hautausschlag', '', 'kwid=3-88', '', 'tid=2', '10-07-17-23', 'pipeline-12-05-17-14', '0990341450874', 'hautausschlag', 'ausschlag', 'exanthem', 'fleckiger', 'hautrötung', 'rote', 'hautflecken', 'definition', 'hautausschlag', 'kann', 'auf', 'eine', 'entzündliche', 'infektiöse', 'oder', 'allergische', 'rekation', 'sein', 'unter', 'einem', 'hautausschlag', 'versteht', 'man', 'eine', 'meist', 'fleckige', 'rötung', 'häufig', 'kombiniert', 'mit', 'einer', 'lokalen', 'verdickung', 'der', 'haut']\n",
      "['rötung', 'und', 'verdickung', 'sind', 'zeichen', 'einer', 'entzündungsreaktion', 'entweder', 'durch', 'eine', 'infektion', 'mit', 'einem', 'mikroorganismus', 'oder', 'durch', 'eine', 'allergische', 'pseudoallergische', 'oder', 'toxische', 'reaktion', 'mit', 'oder', 'ohne', 'juckreiz']\n",
      "['in', 'der', 'fachsprache', 'spricht', 'man', 'von', 'einem', 'exanthem', 'wenn', 'die', 'haut', 'und', 'von', 'einem', 'enanthem', 'wenn', 'die', 'schleimhaut', 'betroffen', 'ist']\n",
      "['wie', 'äussert', 'sich', 'das', 'symptom']\n"
     ]
    }
   ],
   "source": [
    "# inspect and analyze sample documents\n",
    "from random import randint\n",
    "for documents in docs:\n",
    "    document = documents[randint(0, len(documents))]\n",
    "    print(\"\\n== Document ==\")\n",
    "    print(document[:500])\n",
    "    \n",
    "    print(\"== Sentences ==\")\n",
    "    for sentence in extract_sentences(document, sentence_detector)[:4]:\n",
    "        print(sentence)\n",
    "        \n",
    "    print(\"== Tokens ==\")\n",
    "    for sentence in extract_sentences(document, sentence_detector)[:4]:\n",
    "        print(tokenizer.tokenize(sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dewiki-latest-pages-articles.xml.bz2: 4.88GB [39:21, 2.06MB/s]                               \n"
     ]
    }
   ],
   "source": [
    "# download wiki dump. Takes ~ 40min\n",
    "import urllib.request\n",
    "\n",
    "from tqdm import tqdm\n",
    "#from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "class TqdmUpTo(tqdm):\n",
    "    \"\"\"Provides `update_to(n)` which uses `tqdm.update(delta_n)`.\"\"\"\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        \"\"\"\n",
    "        b  : int, optional\n",
    "            Number of blocks transferred so far [default: 1].\n",
    "        bsize  : int, optional\n",
    "            Size of each block (in tqdm units) [default: 1].\n",
    "        tsize  : int, optional\n",
    "            Total size (in tqdm units). If [default: None] remains unchanged.\n",
    "        \"\"\"\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)  # will also set self.n = b * bsize\n",
    "\n",
    "with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=config[\"wiki_dump_url\"].split('/')[-1]) as t:  # all optional kwargs\n",
    "    urllib.request.urlretrieve(config[\"wiki_dump_url\"], config[\"wiki_dump\"], reporthook=t.update_to, data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract wiki dump. Takes ~ 15min\n",
    "\n",
    "from subprocess import call\n",
    "call([\"bzip2\", \"-dk\", config[\"wiki_dump\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract medical and non medical articles from wiki dump\n",
    "\n",
    "# create categories file\n",
    "call([\"ruby\", \"scripts/scan_categories.rb\", config[\"categories_dump_file\"], config['categories_file']])\n",
    "\n",
    "\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert XML to TXT\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train medtext classifier\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# start crawler\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract TXT from PDFs\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train embedding / start medword\n",
    "\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
